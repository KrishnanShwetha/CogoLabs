{"cells":[{"cell_type":"markdown","metadata":{"id":"CZiXWN60BGaO"},"source":["# Cross-validation and intuition\n","\n","In this notebook, we will look at how to do cross-validation in Python and build some intuition about what happens when some of our implicit assumptions about our data breaks down. We will specifically look at what happens when we train on datasets that are too small or non-representative, and look at how to include interaction terms in our models. "]},{"cell_type":"markdown","metadata":{"id":"cm6qfnWICL00"},"source":["## Background\n","\n","We will continue using the dataset provided by Cogo Labs that we've used in previous labs. Recall that we are trying to predict customers' email open rates. "]},{"cell_type":"markdown","metadata":{"id":"LfKyubVDCL3V"},"source":["## Setup\n","\n","Lets start by importing the necessary libraries and mounting the Google Drive:"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":16868,"status":"ok","timestamp":1645500577172,"user":{"displayName":"Gerdus Benade","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10720838145833134874"},"user_tz":300},"id":"GmMIgRcrbmmR","outputId":"774a8016-e7ab-4c09-96bd-0905ff6faac3"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}],"source":["import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import random\n","\n","from sklearn.model_selection import train_test_split\n","from sklearn.linear_model import LinearRegression\n","from sklearn.preprocessing import PolynomialFeatures\n","\n","from sklearn.metrics import mean_squared_error\n","\n","\n","from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","metadata":{"id":"7SxVujQMDMGE"},"source":["## Import the data\n","\n","The dataset we will use is the same that we used for the labs on descriptive analytics; refer to the earlier\n","descriptions for details. Let’s load the training data. Change the path below to accurately reflect the location\n","of the data on your Drive."]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":1622,"status":"ok","timestamp":1645500578791,"user":{"displayName":"Gerdus Benade","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10720838145833134874"},"user_tz":300},"id":"dtEpjcayCmrm"},"outputs":[],"source":["df = pd.read_csv('/content/drive/My Drive/MLBA/cogo-all.tsv', sep='\\t')"]},{"cell_type":"markdown","metadata":{"id":"rELTHzl38uaK"},"source":["## Train and Test Sets\n","\n","We start by splitting the data into a training and testing set. We've done this manually before, but today we'll use Scikit-learn's `train_test_split` function. "]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":190,"status":"ok","timestamp":1645500578979,"user":{"displayName":"Gerdus Benade","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10720838145833134874"},"user_tz":300},"id":"98JIjvUt8lzy"},"outputs":[],"source":["X = df.loc[:, df.columns != \"p_open\"] \n","y = df[\"p_open\"]\n","\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=5)"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":183,"status":"ok","timestamp":1645500579160,"user":{"displayName":"Gerdus Benade","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10720838145833134874"},"user_tz":300},"id":"zpNcsudC_UoF","outputId":"5bdc9d7b-23e8-4034-f07c-3835b96e06cc"},"outputs":[{"data":{"text/plain":["((230638, 16), (57660, 16), (230638,), (57660,))"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["X_train.shape, X_test.shape, y_train.shape, y_test.shape"]},{"cell_type":"markdown","metadata":{"id":"mLU1OFoxBqXd"},"source":["The training data contains 173276 observations. This will slow down training. Let’s take a random subsample\n","of the training set to train on. Then, when we are happy with the tuning of our algorithms, we can increase\n","the size of the training set further. "]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8,"status":"ok","timestamp":1645500579162,"user":{"displayName":"Gerdus Benade","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10720838145833134874"},"user_tz":300},"id":"GU3CE9BwAvGs","outputId":"eeca9e94-9b64-471c-ffec-ac9b88bbf346"},"outputs":[{"data":{"text/plain":["(5000, 17)"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["df_train = X_train.copy()\n","\n","# Add p_open to the combined training dataframe\n","df_train[\"p_open\"] = y_train \n","\n","# Randomly sample 5000 rows from the training data\n","train_sample_size = 5000\n","df_train_sample = df_train.sample(n=train_sample_size, random_state=5) \n","\n","df_train_sample.shape"]},{"cell_type":"markdown","metadata":{"id":"2LyWFyIlfWb-"},"source":["## Data Preparation\n","\n","Instead of training the model on every predictor of the data, we can train it using a subset of the predictors. Lets start by training a linear regression model with three predictors"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":180,"status":"ok","timestamp":1645500579336,"user":{"displayName":"Gerdus Benade","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10720838145833134874"},"user_tz":300},"id":"BWOqlisOeLv4","outputId":"3b72fd65-c237-492e-86b0-69e7856244dd"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:5: FutureWarning: Support for multi-dimensional indexing (e.g. `obj[:, None]`) is deprecated and will be removed in a future version.  Convert to a numpy array before indexing instead.\n","  \"\"\"\n"]},{"data":{"text/plain":["(5000, 1)"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["# it's up to you to add more predictors as you see fit\n","predictors1 = [\"browser1\", \"browser2\", \"browser3\"]\n","\n","X_train_sample_p1 = df_train_sample[predictors1]\n","y_train_sample = df_train_sample[\"p_open\"][:, np.newaxis]\n","\n","X_test_p1 = X_test[predictors1]\n","y_train_sample.shape"]},{"cell_type":"markdown","metadata":{"id":"B_Po3aNajyEI"},"source":["## Linear Regression (recap)\n","\n","We've already seen how to train a linear regression model, inspect its coefficients and compute mean squared error. "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yAnO8ZSEovoD"},"outputs":[],"source":["from sklearn.metrics import mean_squared_error\n","\n","# Create a linear regression object lr_model\n","# Train the model\n","# Predict\n","# Evaluate, store results in mse_train_lr\n","\n","print(\"intercept: \", lr_model.intercept_)\n","print(\"coefficients: \", lr_model.coef_)\n","print(\"train mse: \", mse_train_lr, np.sqrt(mse_train_lr))"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":143,"status":"ok","timestamp":1645500591777,"user":{"displayName":"Gerdus Benade","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10720838145833134874"},"user_tz":300},"id":"9Bw1V4NjganF","outputId":"f87fa3a7-6e56-4706-fd83-be8cc53aafc3"},"outputs":[{"name":"stdout","output_type":"stream","text":["intercept:  [0.07390363]\n","coefficients:  [[0.00719408 0.00413649 0.03834116]]\n","train mse:  0.028116012787471698 0.16767830148075719\n"]}],"source":["from sklearn.metrics import mean_squared_error\n","\n","# Create a linear regression object\n","lr_model = LinearRegression()\n","# Train the model\n","lr_model.fit(X_train_sample_p1, y_train_sample)\n","# Predict\n","yhat_train_lr = lr_model.predict(X_train_sample_p1)\n","# Evaluate\n","mse_train_lr = mean_squared_error(y_train_sample, yhat_train_lr)\n","#mse_train_lr = np.mean((y_train_sample - yhat_train_lr)**2)\n","\n","print(\"intercept: \", lr_model.intercept_)\n","print(\"coefficients: \", lr_model.coef_)\n","print(\"train mse: \", mse_train_lr, np.sqrt(mse_train_lr))"]},{"cell_type":"markdown","metadata":{"id":"EexYEjCtXlxQ"},"source":["So, we have $MSE_{Train} \\approx 0.0281$. It’s a start. Now is a good time to recall that OLS minimizes the sum least of squares, i.e., the following objective function: $MSE_{Train} = \\sum_{i}^{n} (y_i - \\hat{y_i})^2$ where $\\hat{y_i} = \\beta_0 + \\sum_{i}^{p} \\beta_i x_i$.  To get a sense of how accurate our predictions are, we can take the root of the MSE, in this case $\\sqrt{MSE_{Train}} \\approx 0.167$, so on average our prediction of open rate is off by about 17%.\n","\n","We should also compute MSE on the test set, which is what we really care about."]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":187,"status":"ok","timestamp":1645500598494,"user":{"displayName":"Gerdus Benade","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10720838145833134874"},"user_tz":300},"id":"5cDIKZrEZ7mD","outputId":"c85eb77e-8596-4eb9-a63e-7c5de07dcdae"},"outputs":[{"name":"stdout","output_type":"stream","text":["0.028703277011135748\n"]}],"source":["yhat_test_lr = lr_model.predict(X_test_p1)\n","mse_test_lr = mean_squared_error(y_test, yhat_test_lr)\n","print(mse_test_lr)"]},{"cell_type":"markdown","metadata":{"id":"IeKKtCe7bcD-"},"source":["Slightly higher than than our training error, but not so much that we should be worried. "]},{"cell_type":"markdown","metadata":{"id":"ybGQ_tEPRnoL"},"source":["## Cross-validation\n","We discussed how cross-validation gives us more accurate error estimates by repeatedly treating a different subset of our data as validation set. Scikit-learn has built-in functions for cross-validation. \n","\n","We will use the `cross_val_score` function, which has three required arguments when doing supervised learning: a classifier, your data (`X`), and your outcomes (`y`). The optional argument `cv` let's you set the number of folds you want to use. \n","The  `scoring` argument evaluates several known scoring rules automatically so that you don't have to compute the error rate by hand. The available scoring rules are discussed [here](https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter). "]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":133,"status":"ok","timestamp":1645500601762,"user":{"displayName":"Gerdus Benade","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10720838145833134874"},"user_tz":300},"id":"sA97RYq5Kk7D","outputId":"6a4cd714-6b9c-4577-9150-43c04451c138"},"outputs":[{"data":{"text/plain":["array([-0.02349767, -0.02990607, -0.03134196, -0.03032754, -0.02571717])"]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["from sklearn.model_selection import cross_val_score\n","# Instantiate a linear regression model\n","lr_model = LinearRegression()\n","cross_val_score(lr_model, X_train_sample_p1, y_train_sample, cv=5, scoring='neg_mean_squared_error')"]},{"cell_type":"markdown","metadata":{"id":"2NuBUld2TIjG"},"source":["Notice that it returns negative mean squared error by default (but this is easy enough to negate). Now that we have the accuracy on every fold, we can compute our final accuracy estimate. "]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":138,"status":"ok","timestamp":1645500604167,"user":{"displayName":"Gerdus Benade","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10720838145833134874"},"user_tz":300},"id":"h2QDGP3jboUm","outputId":"8e97c5c8-a6c7-4efd-a55a-7c60f0330e18"},"outputs":[{"name":"stdout","output_type":"stream","text":["0.028158083587366184\n"]}],"source":["lr_cv5_scores = cross_val_score(lr_model, X_train_sample_p1, y_train_sample, cv=5, scoring='neg_mean_squared_error')\n","mse_lr_cv5 = np.mean(-lr_cv5_scores)\n","print(mse_lr_cv5)"]},{"cell_type":"markdown","metadata":{"id":"h0Io1VClTlSk"},"source":["### Cross-validation for parameter selection\n","\n","Recall that Lasso and Ridge have a regularization parameter $\\lambda$ which must be tuned. One way to tune this would be to use the `cross_val_score` function several times, once for each of the parameter values you are considering.  You should try to implement this to make sure you understand the steps. "]},{"cell_type":"code","execution_count":12,"metadata":{"executionInfo":{"elapsed":143,"status":"ok","timestamp":1645500638250,"user":{"displayName":"Gerdus Benade","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10720838145833134874"},"user_tz":300},"id":"ya9oeLJgdTh5"},"outputs":[],"source":["# dont need. \n","predictors2 = ['browser1', 'browser2', 'browser3', 'activity_recency', 'activity_observations', 'activity_days', 'activity_locations', 'age' ]\n","X_train_sample_p2 = df_train_sample[predictors2]\n","X_test_p2 = X_test[predictors2]"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":170,"status":"aborted","timestamp":1645500579680,"user":{"displayName":"Gerdus Benade","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10720838145833134874"},"user_tz":300},"id":"5RLCc8h1pcEE"},"outputs":[],"source":["from sklearn.linear_model import Ridge, Lasso\n","\n","lambdas = np.logspace(-3, 7, num=10)\n","cv_scores = []\n","\n","for lam in lambdas: \n","  # instantiate a ridge model with the current lambda (lam)\n","  # do 5-fold cross-validation and store the results in cv_scores\n","\n","plt.plot(lambdas, cv_scores)\n","plt.xscale('log')"]},{"cell_type":"code","execution_count":18,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":269},"executionInfo":{"elapsed":1101,"status":"ok","timestamp":1645504054001,"user":{"displayName":"Gerdus Benade","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10720838145833134874"},"user_tz":300},"id":"mqquJKtqURW6","outputId":"b45a73a9-17cf-4b7e-dce8-793d4500da81"},"outputs":[{"data":{"image/png":"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","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"needs_background":"light"},"output_type":"display_data"}],"source":["from sklearn.linear_model import Ridge, Lasso\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.pipeline import make_pipeline\n","\n","lambdas = np.logspace(-3, 7, num=10)\n","cv_scores = []\n","\n","for l in lambdas: \n","  ridge =  make_pipeline( StandardScaler(), Ridge(alpha=l) )\n","  this_cv_scores = cross_val_score(ridge, X_train_sample_p1, y_train_sample, cv=5, scoring='neg_mean_squared_error')\n","  cv_scores.append(np.mean(-this_cv_scores))\n","\n","plt.plot(lambdas, cv_scores)\n","plt.xscale('log')"]},{"cell_type":"markdown","metadata":{"id":"vDNwRQxYfEHX"},"source":["Now that we have our cross-validation estimates for each parameter, we should train a model on the entire training set using the best parameter. This is the model we will evaluate on the test set. "]},{"cell_type":"code","execution_count":20,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":138,"status":"ok","timestamp":1645504127890,"user":{"displayName":"Gerdus Benade","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10720838145833134874"},"user_tz":300},"id":"eSekiSKgfVfs","outputId":"f9cce343-8482-4436-c39f-761b3d88f482"},"outputs":[{"name":"stdout","output_type":"stream","text":["0.02870780370856948\n"]}],"source":["min_idx = np.argmin(cv_scores)\n","ridge_final = make_pipeline( StandardScaler(), Ridge(alpha=lambdas[min_idx]) )\n","ridge_final.fit(X_train_sample_p1, y_train_sample)\n","yhat_ridge = ridge_final.predict(X_test[predictors1])\n","mse_ridge_test = mean_squared_error(y_test, yhat_ridge)\n","print(mse_ridge_test)"]},{"cell_type":"markdown","metadata":{"id":"XiXwkS85US5o"},"source":["Of course, this is a common task in machine learning, so functions exist to automate it. In particular, you should take a look at  [`LassoCV`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LassoCV.html) and [`RidgeCV`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.RidgeCV.html), which allow you to specify a vector (or number) of $\\lambda$'s (called alphas here) to try as well as the number of folds used for the cross-validation. "]},{"cell_type":"markdown","metadata":{"id":"jjA5IIGu132k"},"source":["## Building intuition: Adding predictors to your model\n","\n","The models we have fit thusfar have been very simple, they only used browser data to predict open rates.\n","Try fitting more complex models by including additional predictors like activity_recency etc., how does\n","this affect $MSE_{Train}$ and $MSE_{Test}$?"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":341,"status":"aborted","timestamp":1645500579852,"user":{"displayName":"Gerdus Benade","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10720838145833134874"},"user_tz":300},"id":"issl79B5XgVz"},"outputs":[],"source":["predictors2 = ['browser1', 'browser2', 'browser3', 'activity_recency', 'activity_observations', 'activity_days', 'activity_locations', 'age' ]\n","\n","X_train_sample_p2 = df_train_sample[predictors2]\n","y_train_sample = df_train_sample[\"p_open\"]\n","\n","lr_model = LinearRegression()\n","lr_model.fit( ... )\n","\n","#evaluate on training set\n","\n","#evaluate on test set\n","\n","print(mse_train_lr_p2, mse_test_lr_p2)"]},{"cell_type":"code","execution_count":22,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":119,"status":"ok","timestamp":1645504152698,"user":{"displayName":"Gerdus Benade","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10720838145833134874"},"user_tz":300},"id":"u0Tifzq6qh8l","outputId":"5a7d498e-68df-4518-8132-07058be67c3b"},"outputs":[{"name":"stdout","output_type":"stream","text":["0.027685618004062832 0.028445095988509025\n"]}],"source":["predictors2 = ['browser1', 'browser2', 'browser3', 'activity_recency', 'activity_observations', 'activity_days', 'activity_locations', 'age' ]\n","\n","X_train_sample_p2 = df_train_sample[predictors2]\n","y_train_sample = df_train_sample[\"p_open\"]\n","\n","lr_model = make_pipeline( StandardScaler(), LinearRegression())\n","lr_model.fit(X_train_sample_p2, y_train_sample)\n","\n","#evaluate on training set\n","yhat_train_lr = lr_model.predict(X_train_sample_p2)\n","mse_train_lr = np.mean(np.square(y_train_sample - yhat_train_lr))\n","\n","#evaluate on test set\n","yhat_test_lr = lr_model.predict(X_test[predictors2])\n","mse_test_lr = np.mean(np.square(y_test - yhat_test_lr))\n","\n","print(mse_train_lr, mse_test_lr)"]},{"cell_type":"markdown","metadata":{"id":"A8eyO9DCi7B4"},"source":["Notice this causes a significant improvement in training error, but the improvement in testing error is much smaller. We may even be overfitting a little here.  "]},{"cell_type":"markdown","metadata":{"id":"PyLa4lAx28hI"},"source":["You can also add new predictors to the data set, for example, recall that we saw a clear divide in open rates\n","by state. Perhaps adding a predictor that says whether a state is ‘high’ or ‘low’ will help."]},{"cell_type":"markdown","metadata":{"id":"UhFD9uY2ZEF6"},"source":["## Building intuition: Training on a small sample\n","\n","What do you expect would happen if we trained on a very small sample of the training set, say only 30\n","observations? The following code takes a subset of the data."]},{"cell_type":"code","execution_count":23,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":120,"status":"ok","timestamp":1645504162811,"user":{"displayName":"Gerdus Benade","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10720838145833134874"},"user_tz":300},"id":"j430qhBK2raK","outputId":"8e08039e-54bc-4084-c9cd-af590a221c7d"},"outputs":[{"data":{"text/plain":["(30, 17)"]},"execution_count":23,"metadata":{},"output_type":"execute_result"}],"source":["train_sample_size = 30\n","df_train_small_sample = df_train.sample(n=train_sample_size, random_state=5)\n","df_train_small_sample.shape"]},{"cell_type":"markdown","metadata":{"id":"wRLnbdROZED6"},"source":["Now train a model using one of your formulae and compute its accuracy. How does training on a small\n","sample affect $MSE_{Train}$ and $MSE_{Test}$?"]},{"cell_type":"code","execution_count":24,"metadata":{"executionInfo":{"elapsed":131,"status":"ok","timestamp":1645504164261,"user":{"displayName":"Gerdus Benade","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10720838145833134874"},"user_tz":300},"id":"Y1w4bTSXiNgU"},"outputs":[],"source":["X_train_small_sample = df_train_small_sample[predictors1]\n","y_train_small_sample = df_train_small_sample['p_open']"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":185,"status":"aborted","timestamp":1645500579855,"user":{"displayName":"Gerdus Benade","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10720838145833134874"},"user_tz":300},"id":"ZGCtyTIuq7Ia"},"outputs":[],"source":["# fit a linear regression on small training set, evaluate on train + test\n"]},{"cell_type":"code","execution_count":25,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":155,"status":"ok","timestamp":1645504165399,"user":{"displayName":"Gerdus Benade","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10720838145833134874"},"user_tz":300},"id":"wyQHI5pklCtb","outputId":"1c99a10e-f380-4d06-fabc-a0dfb3baa55b"},"outputs":[{"name":"stdout","output_type":"stream","text":["0.010277914277443258 0.0318269937980886\n"]}],"source":["lr_model.fit(X_train_small_sample, y_train_small_sample)\n","yhat_train_sample = lr_model.predict(X_train_small_sample)\n","yhat_test_sample = lr_model.predict(X_test[predictors1])\n","print (mean_squared_error(y_train_small_sample, yhat_train_sample), mean_squared_error(y_test, yhat_test_sample))"]},{"cell_type":"markdown","metadata":{"id":"lbXPgYtcZEA5"},"source":["## Building intuition: The importance of randomness\n","\n","To illustrate the importance of training on a random sample of the data, let’s train on a non-representative\n","sample found by using an age cutoff."]},{"cell_type":"code","execution_count":26,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":146,"status":"ok","timestamp":1645504193105,"user":{"displayName":"Gerdus Benade","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10720838145833134874"},"user_tz":300},"id":"wl4tdc4p2kTW","outputId":"1edf0526-bb60-4b9c-c52e-453938c5d970"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n","  \"\"\"Entry point for launching an IPython kernel.\n"]}],"source":["df_train_nr = df_train[df.age > 26]\n","X_train_nr = df_train_nr.loc[:, df.columns != \"p_open\"]\n","y_train_nr = df_train_nr[\"p_open\"]"]},{"cell_type":"markdown","metadata":{"id":"o0rfcBw8bWh5"},"source":["For the sake of comparison, we will take a random sample of the same size. Since we are sampling randomly, we expect this sample to be respresentative of our testing data. "]},{"cell_type":"code","execution_count":28,"metadata":{"executionInfo":{"elapsed":307,"status":"ok","timestamp":1645504217513,"user":{"displayName":"Gerdus Benade","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10720838145833134874"},"user_tz":300},"id":"62Um-P95bDKr"},"outputs":[],"source":["train_sample_size = len(X_train_nr)\n","\n","df_train = X_train.copy()\n","df_train[\"p_open\"] = y_train # Add p_open to the combined training dataframe\n","df_train_sample = df_train.sample(n=train_sample_size, random_state=5)\n","\n","X_train_r = df_train_sample.loc[:, df.columns != \"p_open\"]\n","y_train_r = df_train_sample[\"p_open\"]"]},{"cell_type":"markdown","metadata":{"id":"AJ0XpWYsbWU3"},"source":["We will evaluate the two models trained on these different training samples on the same test set.\n","\n","Begin by using one of your models, say f1, to train on the non-random sample and compute $MSE_{Train}$ and $MSE_{Test}$."]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":186,"status":"aborted","timestamp":1645500579856,"user":{"displayName":"Gerdus Benade","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10720838145833134874"},"user_tz":300},"id":"nPLZCzZibUEo"},"outputs":[],"source":["X_train_nr_p1 = X_train_nr[predictors1]\n","X_train_r_p1 = X_train_r[predictors1]\n","\n","lr_model = LinearRegression()\n","lr_model.fit(... , ...)\n","\n","yhat_train_nr = # your code here\n","mse_train_nr = #...\n","yhat_test_nr = #...\n","mse_test_nr = #..."]},{"cell_type":"markdown","metadata":{"id":"DxDjMqnddZsx"},"source":["Now do the same for the random sample."]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":186,"status":"aborted","timestamp":1645500579856,"user":{"displayName":"Gerdus Benade","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10720838145833134874"},"user_tz":300},"id":"h88sS1wjdoxg"},"outputs":[],"source":["lr_model_r.fit ... , ...)\n","\n","yhat_train_r = # your code here\n","mse_train_r = #...\n","yhat_test_r = #...\n","mse_test_r = #..."]},{"cell_type":"markdown","metadata":{"id":"l8E25eywdZei"},"source":["What do you observe? Can you explain why?"]},{"cell_type":"code","execution_count":31,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":148,"status":"ok","timestamp":1645504255616,"user":{"displayName":"Gerdus Benade","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10720838145833134874"},"user_tz":300},"id":"LOd4AqqHluL9","outputId":"a38e7234-b55c-403b-a7ee-4711da1b1972"},"outputs":[{"name":"stdout","output_type":"stream","text":["0.03464807999739183 0.029248142461207546\n"]}],"source":["X_train_nr_p1 = X_train_nr[predictors1]\n","X_train_r_p1 = X_train_r[predictors1]\n","\n","lr_model_nr = LinearRegression()\n","lr_model_nr.fit(X_train_nr_p1, y_train_nr)\n","\n","yhat_train_nr = lr_model_nr.predict(X_train_nr_p1)\n","mse_train_nr = mean_squared_error(y_train_nr, yhat_train_nr)\n","yhat_test_nr = lr_model_nr.predict(X_test_p1)\n","mse_test_nr = mean_squared_error(y_test, yhat_test_nr)\n","print(mse_train_nr, mse_test_nr)"]},{"cell_type":"code","execution_count":32,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":282,"status":"ok","timestamp":1645504257086,"user":{"displayName":"Gerdus Benade","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10720838145833134874"},"user_tz":300},"id":"Rf6bg3F3lvzq","outputId":"e3666eb1-8e45-49c3-d062-4d8b7a5c042e"},"outputs":[{"name":"stdout","output_type":"stream","text":["0.027806356541009415 0.028692178459380525\n"]}],"source":["lr_model_r = LinearRegression()\n","lr_model_r.fit(X_train_r_p1, y_train_r)\n","\n","yhat_train_r = lr_model_r.predict(X_train_r_p1)\n","mse_train_r = mean_squared_error(y_train_r, yhat_train_r)\n","yhat_test_r = lr_model_r.predict(X_test_p1)\n","mse_test_r = mean_squared_error(y_test, yhat_test_r)\n","print(mse_train_r, mse_test_r)"]},{"cell_type":"markdown","metadata":{"id":"35J6R70bmsbF"},"source":["Notice that testing error is lower when we train on a random sample, so our model generalizes better. "]},{"cell_type":"markdown","metadata":{"id":"s9EFThasdkix"},"source":["## Building intuition: Interactions between predictors\n","\n","Sometimes we may think that there is some sort of interaction between our variables. For example perhaps we\n","suspect that college-aged users who have a low number of activity locations behave similarly to post-college\n","aged users with a high number of activity locations. In this case a linear model based on `activity_locations`\n","and age will be confused - if an increase in activity_days leads to an increase in `p_open` when age is\n","greater than 21, but leads to a decrease when age is less than 21, what should the sign on the coefficent of\n","`activity_locations` be in the model?\n","\n","If we create a new variable, `(age-21)`·`(activity_locations-8)` it will be positive for highly-mobile post-\n","college-aged users and static college-aged users. It will be negative for static post-college-aged users and highly-mobile college-aged users. If we already have the linear terms incorperated in our model, we can\n","capture this behavior by simply adding the interaction term, without having to define a new variable.\n","\n","We do this with the same `PolynomialFeatures` transformer we've used before.  In this case we will set `interaction_only` to `True`, and use a second-degree polynoial: the effect of this is that we get all the terms which are products of at most two *distinct* terms. "]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":187,"status":"aborted","timestamp":1645500579857,"user":{"displayName":"Gerdus Benade","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10720838145833134874"},"user_tz":300},"id":"SqIQx0Pxeqtf"},"outputs":[],"source":["x_train_sample = df_train_sample[[\"activity_locations\", \"age\"]]\n","interaction = PolynomialFeatures(degree=2, include_bias=False, interaction_only=True)\n","x_train_inter = interaction.fit_transform(x_train_sample)\n","x_train_inter.shape"]},{"cell_type":"markdown","metadata":{"id":"e_rEcpFZfw_N"},"source":["Use `x_train_inter` to train a linear regression model. Compute $MSE_{Train}$ and $MSE_{Test}$ for this model. Create another model with several more interaction terms.\n","Remember we can look at the parameters of a model by using `coef_`.\n","\n","What do you think the sign of the coefficient of the interaction term means?\n","\n","Try running the different models with lasso and ridge model to see if they exhibit similar behavior."]}],"metadata":{"colab":{"collapsed_sections":[],"name":"Lab3-CV-and-intuition-solutions.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3.8.2 64-bit","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.8.2"},"vscode":{"interpreter":{"hash":"31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"}}},"nbformat":4,"nbformat_minor":0}
