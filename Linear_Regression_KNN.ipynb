{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{"id":"4AI6kCjUdadG"},"source":["# Linear regression and KNN\n","\n"]},{"cell_type":"markdown","metadata":{"id":"j_jWf_sDdxk6"},"source":["## Setup\n","\n","Lets start by importing the packages we'll need and mounting our Google Drive as before. "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZNxLcICQuclV"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","metadata":{"id":"D9Xg2nxHd-b7"},"source":["We'll use the `read_csv` function to read the dataset, be sure to take a look at the [documentation](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html). There are mamy optional arguments you may find useful when working on your project dataset. "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xqONUClOu9vo"},"outputs":[],"source":["df = pd.read_csv('/content/drive/My Drive/MLBA/cogo-all.tsv', delimiter='\\t')"]},{"cell_type":"markdown","metadata":{"id":"0OB4IU45eY4T"},"source":["We have to split the data into a training and testing set. `sklearn.model_selection` offers us automated ways of doing this which we will use in the future but since this is our first time, let's do it manually. \n","\n","We create a new column called `train` which is `True` if the instance should be included in the training by using the numpy random number generator. Once we have this column, we can filter on it to create two new dataframes. "]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"elapsed":835,"status":"ok","timestamp":1601607566345,"user":{"displayName":"Johannes Gerhardus Benade","photoUrl":"","userId":"10720838145833134874"},"user_tz":240},"id":"iqYd7CNywwVO","outputId":"fbaf3db0-995f-4474-c83a-9e78f321777e"},"outputs":[{"name":"stdout","output_type":"stream","text":["(230660, 18) (57638, 18)\n"]}],"source":["df['train'] = np.random.rand(len(df)) < 0.8\n","\n","df_train = df[df.train == True]\n","df_test = df[df.train == False]\n","\n","print(df_train.shape, df_test.shape)"]},{"cell_type":"markdown","metadata":{"id":"Hg5CwMumfQjJ"},"source":["We can refresh our memories of what goes on in the dataset by looking at the column names. "]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":102},"executionInfo":{"elapsed":832,"status":"ok","timestamp":1601607566346,"user":{"displayName":"Johannes Gerhardus Benade","photoUrl":"","userId":"10720838145833134874"},"user_tz":240},"id":"E3SCMbcXAWTX","outputId":"b5177f4e-7727-4115-bfa3-b9a10b1ae765"},"outputs":[{"data":{"text/plain":["Index(['state', 'user_id', 'browser1', 'browser2', 'browser3', 'device_type1',\n","       'device_type2', 'device_type3', 'device_type4', 'activity_observations',\n","       'activity_days', 'activity_recency', 'activity_locations',\n","       'activity_ids', 'age', 'gender', 'p_open', 'train'],\n","      dtype='object')"]},"execution_count":30,"metadata":{"tags":[]},"output_type":"execute_result"}],"source":["df.columns"]},{"cell_type":"markdown","metadata":{"id":"9s5oNkZ-fHTs"},"source":["## Training a linear regression model"]},{"cell_type":"markdown","metadata":{"id":"ou8rI9j0f5f5"},"source":["Let's start by setting up a very simple model that only cares about which browser a user uses. We will create a list, `predictors1` to hold ne column names of the predictors we want to include in our model to make indexing easier."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MHC31vFIgj84"},"outputs":[],"source":["predictors = ['browser1', 'browser2', 'browser3']\n","X1_train = df_train[predictors]\n","X1_test = df_test[predictors]\n","y_train = df_train['p_open']\n","y_test = df_test['p_open']"]},{"cell_type":"markdown","metadata":{"id":"KHaZtj6cgrWi"},"source":["Now we can follow the same four steps as always. First, choose a model class, instantiante the model and set hyperparameter values, then fit to your data. Remember we can access model attributes, in this case the coefficients and intercepts term.  "]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"elapsed":953,"status":"ok","timestamp":1601607566486,"user":{"displayName":"Johannes Gerhardus Benade","photoUrl":"","userId":"10720838145833134874"},"user_tz":240},"id":"ZKv-j8ctxqMi","outputId":"4d25125d-93b8-4ae7-d32b-2de33aac8d4f"},"outputs":[{"data":{"text/plain":["(array([0.01618858, 0.00640617, 0.04715025]), 0.07030346777090489)"]},"execution_count":32,"metadata":{"tags":[]},"output_type":"execute_result"}],"source":["from sklearn.linear_model import LinearRegression # 1. choose model class\n","model = LinearRegression(fit_intercept=True)      # 2. instantiate model\n","model.fit(X1_train, y_train)                      # 3. fit model to data\n","\n","model.coef_, model.intercept_"]},{"cell_type":"markdown","metadata":{"id":"JzwefjGYhlyz"},"source":["Finally, we make predictions on the training and test set and evaluate the mean squared error. Of course, there are automated functions for this, but let's do it manually so that we can make sure we understand how it works. \n","\n","First we predict on the training data:"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"elapsed":950,"status":"ok","timestamp":1601607566486,"user":{"displayName":"Johannes Gerhardus Benade","photoUrl":"","userId":"10720838145833134874"},"user_tz":240},"id":"iME4cItosYor","outputId":"649dee44-41d1-4f63-cf8f-d7232cfb75e0"},"outputs":[{"name":"stdout","output_type":"stream","text":["0.16881952807394998 0.028500033059111182\n"]}],"source":["y_train_fit = model.predict(X1_train)              # 4a. predict on training data\n","\n","mse_train = np.mean( (y_train - y_train_fit)**2 )\n","print(np.sqrt(mse_train), mse_train)"]},{"cell_type":"markdown","metadata":{"id":"0czfTAwGs5Wm"},"source":["Then on the testing data:"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"elapsed":947,"status":"ok","timestamp":1601607566487,"user":{"displayName":"Johannes Gerhardus Benade","photoUrl":"","userId":"10720838145833134874"},"user_tz":240},"id":"5H6Qo8jNtmN1","outputId":"1d20ce0a-91c7-42aa-9d51-6f2336d11dcf"},"outputs":[{"name":"stdout","output_type":"stream","text":["0.16956802938033952 0.028753316587931687\n"]}],"source":["y_test_fit = model.predict(X1_test)                # 4b. predict on test data\n","mse_test = np.mean( (y_test - y_test_fit)**2 )\n","print(np.sqrt(mse_test), mse_test)"]},{"cell_type":"markdown","metadata":{"id":"Si6Ov0yps8Rl"},"source":["In this case our MSE is pretty similar, so it's unlikely we overfit. Is this a \"good\" MSE? We don't really know, but we can say that our open-rate predictions are, on average, off by about 17\\%. \n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## KNN"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Use the same steps as above to train a KNN regression, and compute MSE train and MSE test. How do they compare to linear regression?"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from sklearn import neighbors                           # 1. choose model class\n","n_neighbors = 5\n","model = neighbors.KNeighborsRegressor(n_neighbors)      # 2. instantiate model\n","model.fit(X1_train, y_train)                            # 3. fit model to data\n","\n","y_train_fit = model.predict(X1_train)                   # 4a. predict on training data\n","\n","mse_train = np.mean( (y_train - y_train_fit)**2 )\n","print(np.sqrt(mse_train), mse_train)\n","\n","y_test_fit = model.predict(X1_test)                     # 4b. predict on test data\n","mse_test = np.mean( (y_test - y_test_fit)**2 )\n","print(np.sqrt(mse_test), mse_test)"]},{"cell_type":"markdown","metadata":{"id":"SCl_soyWIyqY"},"source":["## Categorical predictors and polynomial basis functions\n","\n","All of the features we used above contained numerical values, but what if this is not the case? Now we need a way to encode categorical values to numbers. This is typically done with one-hot encoding. For example, a column `gender` with three possible values (male, female, other) will be transformed into three binary columns, one for each possible categorical value. We will use the [`OneHotEncoder`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html) for this.\n","\n","Another common transformation is to use polynomial basis functions. Suppose we have predictors $X_1, X_2$. Instead of fitting the linear model $y=\\beta_0 + \\beta_1 X_1 + \\beta_2 X_2$, we may want to fit the (still linear) $y=\\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\beta_3 X_1X_2 + \\beta_4X_1^2 + \\beta_5X_2^2.$ To do this we must add the columns $X_1\\cdot X_2, X_1^2$ and $X_2$ to the data matrix. We will use [`PolynomialFeatures`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html) to add these columns automatically, notice that you can pass the maximum degree when instantiating the transformer. \n"]},{"cell_type":"markdown","metadata":{"id":"ZQT1z5OsL74I"},"source":["The example that follows creates a `ColumnTransformer` to transform to do preprocessing on the five feature columns. The three browser columns are left unchanged and kept (if we did not specify `remainder='passthrough'` they would have been discarded); the `gender` column is transformed to two binary columns, one for each of the genders that appear in the data; and we create quadratic (degree 2 polynomial) features for the  `activity_days` column. "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0QB5LKcn7DIh"},"outputs":[],"source":["from sklearn.compose import ColumnTransformer\n","from sklearn.preprocessing import OneHotEncoder\n","from sklearn.preprocessing import PolynomialFeatures\n","\n","# all predictors \n","numerical_predictors = ['browser1', 'browser2', 'browser3']\n","categorical_predictors = ['gender'] #['gender', 'state']\n","poly_predictors = ['activity_days']\n","all_predictors = numerical_predictors + categorical_predictors + poly_predictors\n","\n","# list of the two transformation we want to do\n","t = [('cat', OneHotEncoder(), categorical_predictors), \n","     ('poly', PolynomialFeatures(2, include_bias=False), poly_predictors)]\n","\n","# instantiate columntransformer with our transforamtions t\n","col_transform = ColumnTransformer(transformers=t, remainder='passthrough')"]},{"cell_type":"markdown","metadata":{"id":"VMNrf7pZNILE"},"source":["Now we can apply the transformation to our training and testing sets. Notice that we have 7 columns after transformation: 3 for the browsers; 2 for the gender one-hot encoding and 2 for `activity_days` and `activity_days`$^2$."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"elapsed":346,"status":"ok","timestamp":1601608108472,"user":{"displayName":"Johannes Gerhardus Benade","photoUrl":"","userId":"10720838145833134874"},"user_tz":240},"id":"ORi-uzoK-SVA","outputId":"83603721-9dd5-4e07-b97c-33cddbeeacab"},"outputs":[{"data":{"text/plain":["(230660, 7)"]},"execution_count":58,"metadata":{"tags":[]},"output_type":"execute_result"}],"source":["xt_train = col_transform.fit_transform(df_train[all_predictors])\n","xt_test = col_transform.fit_transform(df_test[all_predictors])\n","xt_train.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"elapsed":291,"status":"ok","timestamp":1601608110471,"user":{"displayName":"Johannes Gerhardus Benade","photoUrl":"","userId":"10720838145833134874"},"user_tz":240},"id":"Beg402dq91Ax","outputId":"b82ce92d-942e-47e7-b99c-2ae55314e5b3"},"outputs":[{"name":"stdout","output_type":"stream","text":["0.028583370783762215\n"]}],"source":["model = LinearRegression(fit_intercept=True)\n","model.fit(xt_train, y_train)\n","yhat_test = model.predict(xt_test)\n","mse_test = np.mean( (y_test - yhat_test)**2)\n","print(mse_test)"]}],"metadata":{"colab":{"collapsed_sections":[],"name":"Lab2-Regression-solutions.ipynb","provenance":[]},"kernelspec":{"display_name":"base","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.8.10 | packaged by conda-forge | (default, May 11 2021, 06:25:23) [MSC v.1916 64 bit (AMD64)]"},"vscode":{"interpreter":{"hash":"8ddc49676faceb3e8cb567e61f8f552440a95f15ba10e7733a70b03786a0351b"}}},"nbformat":4,"nbformat_minor":0}
